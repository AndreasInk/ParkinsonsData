"""parkinsons_random_forest_1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KfnHA-vWji9aN9oEPsyBuSzAxFvGCTaG
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

df = pd.read_csv("mycsv.csv")
df.head()

# convert to one hot if necessary
df = pd.get_dummies(df)
df.head()

# what to predict
labels = np.array(df["sourceName"])

feature_list = list(df.columns)
df.drop(["sourceName"], axis=1, inplace=True)
df.head()

df = np.array(df)
# df[:10]  # view

# Split the data into training and testing sets
train_features, test_features, train_labels, test_labels = train_test_split(
    df, labels, test_size=0.25, random_state=42
)

# Instantiate model
rf = RandomForestRegressor(n_estimators=1000, random_state=42)

# Train the model on training data
rf.fit(train_features, train_labels)

# Use the forest's predict method on the test data
predictions = rf.predict(test_features)

# Calculate the absolute errors
errors = abs(predictions - test_labels)

print("Mean error:", round(np.mean(errors), 2))

# Calculate mean absolute percentage error (MAPE)
mape = 100 * (errors / test_labels)

# Calculate and display accuracy
accuracy = 100 - np.mean(mape)
print("Accuracy:", round(accuracy, 2), "%.")

# Get numerical feature importances
importances = list(rf.feature_importances_)

# List of tuples with variable and importance
feature_importances = [
    (feature, round(importance, 2))
    for feature, importance in zip(feature_list, importances)
]

# Sort the feature importances by most important first
feature_importances = sorted(feature_importances, key=lambda x: x[1], reverse=True)

# Print out the feature and importances
[print("Variable: {:20} Importance: {}".format(*pair)) for pair in feature_importances]
